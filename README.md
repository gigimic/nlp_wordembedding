Word embedding layers for deep learning with Keras
--------------------------------------------------

https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/

Word embedding is represented as dense vectors as opposed to the sparse matrices in BOW.

Popular methods are:

word2vec & 
GloVe (Global vectors for word representation)

Here we learn 
i) word embedding using keras while fitting a neural network on a text classification problem.
ii) how to use pre-trained GloVe embedding

